{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb024dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f9ec5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagshub\n",
    "import datetime\n",
    "import mlflow\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=\"aRTRw0ORiLGJdj7RF2gR\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"tennis-player-and-tennis-ball-ei0vm\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"yolov12\", location=\"datasets/object-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584ffa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in datasets/object-detection-tile to yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 503644/503644 [00:31<00:00, 15756.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to datasets/object-detection-tile in yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21068/21068 [00:07<00:00, 2822.45it/s]\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=\"aRTRw0ORiLGJdj7RF2gR\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"tennis-player-and-tennis-ball-ei0vm\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"yolov12\",location=\"datasets/object-detection-tile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a91ed5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nugrohorahmanto24\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as nugrohorahmanto24\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abiyamf/courtPlay\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"abiyamf/courtPlay\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abiyamf/courtPlay initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository abiyamf/courtPlay initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dagshub.init(repo_owner='abiyamf', repo_name='courtPlay', mlflow=True)\n",
    "settings.update({\"mlflow\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7ba0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ”§ Global Config\n",
    "# ==========================\n",
    "os.environ[\"MLFLOW_KEEP_RUN_ACTIVE\"] = \"True\"\n",
    "\n",
    "time_now = datetime.datetime.now().strftime(\"%m-%d_%H-%M\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "# Project Info\n",
    "projectType = \"object-detection\"\n",
    "datasetVersion = \"6\"\n",
    "yoloVersion = \"yolo11\"\n",
    "modelSize = \"n\"\n",
    "\n",
    "# Training Params\n",
    "epochs = 50\n",
    "batch = 16\n",
    "imgsz = 720\n",
    "data_path = \"datasets/object-detection/data.yaml\"\n",
    "project_path = f\"results/object-detection/{modelSize}\"\n",
    "\n",
    "# Toggle custom augmentation\n",
    "use_custom_aug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23a3ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds into 'Hh Mm Ss' format.\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "def save_class_metrics(val_results):\n",
    "    # Ambil nama class\n",
    "    names = val_results.names\n",
    "\n",
    "    # Data per-class\n",
    "    data = {\n",
    "        \"class\": [names[i] for i in range(len(names))],\n",
    "        \"precision\": val_results.box.p.tolist(),\n",
    "        \"recall\": val_results.box.r.tolist(),\n",
    "        \"mAP50\": val_results.box.ap50.tolist(),\n",
    "        \"mAP50-95\": val_results.box.maps.tolist(),\n",
    "    }\n",
    "\n",
    "    # Tambahkan baris agregat \"all\"\n",
    "    all_row = {\n",
    "        \"class\": \"all\",\n",
    "        \"precision\": float(val_results.box.mp),\n",
    "        \"recall\": float(val_results.box.mr),\n",
    "        \"mAP50\": float(val_results.box.map50),\n",
    "        \"mAP50-95\": float(val_results.box.map),\n",
    "    }\n",
    "\n",
    "    for key in data.keys():\n",
    "        data[key].insert(0, all_row[key])\n",
    "\n",
    "    # Buat DataFrame dan simpan\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(str(val_results.save_dir), \"class_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4bb69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.204 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.179  Python-3.10.18 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/object-detection/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=10-05_14-32, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=results/object-detection/n, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=results\\object-detection\\n\\10-05_14-32, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431062  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,230 parameters, 2,590,214 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 2142.6597.6 MB/s, size: 187.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Documents\\ABIYA NITIP\\TelU-Tubes-BisnisDigital-CourtPlay-AIDevelopment\\ObjectDetection\\datasets\\object-detection\\train\\labels.cache... 3686 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3686/3686 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1892.0464.9 MB/s, size: 197.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Documents\\ABIYA NITIP\\TelU-Tubes-BisnisDigital-CourtPlay-AIDevelopment\\ObjectDetection\\datasets\\object-detection\\valid\\labels.cache... 527 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 527/527 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to results\\object-detection\\n\\10-05_14-32\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(73687966e7e54a8d9d9adf3836c47469) to https://dagshub.com/abiyamf/courtPlay.mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mresults\\object-detection\\n\\10-05_14-32\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      2.71G      1.188      1.285     0.9442         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:37<00:00,  6.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        527       1962      0.771      0.627      0.629      0.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      2.71G      1.086      0.741     0.9196         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:33<00:00,  6.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:02<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        527       1962      0.811      0.691      0.696      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.023 hours.\n",
      "Optimizer stripped from results\\object-detection\\n\\10-05_14-32\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from results\\object-detection\\n\\10-05_14-32\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating results\\object-detection\\n\\10-05_14-32\\weights\\best.pt...\n",
      "Ultralytics 8.3.179  Python-3.10.18 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:02<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        527       1962      0.807      0.691      0.696      0.415\n",
      "                  ball        408        414      0.668      0.394      0.402      0.117\n",
      "                player        526       1548      0.947      0.989      0.989      0.714\n",
      "Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mresults\\object-detection\\n\\10-05_14-32\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mmlflow run still alive, remember to close it using mlflow.end_run()\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to https://dagshub.com/abiyamf/courtPlay.mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "ðŸƒ View run 10-05_14-32 at: https://dagshub.com/abiyamf/courtPlay.mlflow/#/experiments/6/runs/73687966e7e54a8d9d9adf3836c47469\n",
      "ðŸ§ª View experiment at: https://dagshub.com/abiyamf/courtPlay.mlflow/#/experiments/6\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸš€ Training Routine\n",
    "# ==========================\n",
    "model = YOLO(f\"{yoloVersion}{modelSize}.pt\")\n",
    "mlflow.set_experiment(f\"courtPlay-{modelSize}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Set augmentations (default vs custom)\n",
    "augmentations = {}\n",
    "if use_custom_aug:\n",
    "    augmentations = {\n",
    "        \"mosaic\": 0.4,\n",
    "        \"scale\": 0.25,\n",
    "        \"translate\": 0.05,\n",
    "        \"hsv_s\": 0.4,\n",
    "        \"hsv_v\": 0.2,\n",
    "        \"mixup\": 0.1,\n",
    "        \"flipud\": 0.0,\n",
    "        \"cutmix\": 0.0,\n",
    "        \"shear\": 0.0,\n",
    "        \"perspective\": 0.0,\n",
    "    }\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{time_now}\"):\n",
    "\n",
    "    # Log global parameters\n",
    "    mlflow.log_params({\n",
    "        \"project type\": projectType,\n",
    "        \"dataset version\": datasetVersion,\n",
    "        \"yolo version\": yoloVersion,\n",
    "        \"model size\": modelSize,\n",
    "        \"gpu\": gpu_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch\": batch,\n",
    "        \"imgsz\": imgsz,\n",
    "        \"use_custom_aug\": use_custom_aug,\n",
    "    })\n",
    "\n",
    "    # Train\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=f\"{time_now}\",\n",
    "        project=project_path,\n",
    "        exist_ok=True,\n",
    "        **augmentations\n",
    "    )\n",
    "\n",
    "    # Log training time\n",
    "    mlflow.log_params({\n",
    "        \"training time\": format_duration(time.time() - start_time),\n",
    "    })\n",
    "\n",
    "    # Run validation + log class metrics\n",
    "    mlflow.log_artifact(save_class_metrics(results), artifact_path=\"class_metrics\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a7574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abiya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
